{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nat_rng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nat_rng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nat_rng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nat_rng/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import string\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from urllib.parse import unquote\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "WIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "WIKIPEDIA_API_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"uni_coursework/2.0; nat.1.roongjirarat@kcl.ac.uk\"}\n",
    "\n",
    "PARAMS_QUERY_SEARCH = {\n",
    "    \"action\":\"query\",\n",
    "    \"format\":\"json\",\n",
    "    \"formatversion\":\"latest\",\n",
    "    \"list\":\"search\",\n",
    "    \"srsearch\": \"haswbstatement:P166=Q185667\",\n",
    "    \"srlimit\":\"max\"\n",
    "}\n",
    "\n",
    "PARAMS_GETCONTENT = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",\n",
    "    \"prop\": \"extracts\",\n",
    "    \"exlimit\": \"max\"\n",
    "}\n",
    "\n",
    "PARAMS_WBGETENTITIES_LABELS = {\n",
    "    \"action\": \"wbgetentities\",\n",
    "    \"format\": \"json\",\n",
    "    \"ids\": \"\",\n",
    "    \"sites\": \"\",\n",
    "    \"props\": \"labels\",\n",
    "    \"languages\": \"en\",\n",
    "    \"sitefilter\": \"enwiki\",\n",
    "    \"utf8\": 1,\n",
    "    \"ascii\": 1,\n",
    "    \"formatversion\": \"latest\"\n",
    "}\n",
    "\n",
    "PARAMS_WBGETENTITIES_SITES = {\n",
    "    \"action\": \"wbgetentities\",\n",
    "    \"format\": \"json\",\n",
    "    \"ids\": \"\",\n",
    "    \"sites\": \"\",\n",
    "    \"props\": \"sitelinks/urls\",\n",
    "    \"languages\": \"en\",\n",
    "    \"sitefilter\": \"enwiki\",\n",
    "    \"utf8\": 1,\n",
    "    \"ascii\": 1,\n",
    "    \"formatversion\": \"latest\"\n",
    "}\n",
    "\n",
    "PARAMS_WBGETENTITIES_CLAIMS = {\n",
    "    \"action\": \"wbgetentities\",\n",
    "    \"format\": \"json\",\n",
    "    \"ids\": \"\",\n",
    "    \"props\": \"claims\",\n",
    "    \"languages\": \"en\",\n",
    "    \"sitefilter\": \"\",\n",
    "    \"formatversion\": \"latest\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_turing_award_recipients():\n",
    "    acm_award_entities = []\n",
    "    search_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_QUERY_SEARCH)\n",
    "    data = search_response.json()\n",
    "    for result in data['query']['search']:\n",
    "        acm_award_entities.append(result['title'])\n",
    "    return acm_award_entities\n",
    "\n",
    "print(get_turing_award_recipients())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(entity_id):\n",
    "    PARAMS_WBGETENTITIES_SITES[\"ids\"] = entity_id\n",
    "    wbgetentities_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_WBGETENTITIES_SITES)\n",
    "    wbgetentities_data = wbgetentities_response.json()\n",
    "    recipient_name = wbgetentities_data[\"entities\"][entity_id][\"sitelinks\"][\"enwiki\"][\"url\"].split(\"https://en.wikipedia.org/wiki/\")[1]\n",
    "\n",
    "    PARAMS_GETCONTENT[\"titles\"] = unquote(recipient_name)\n",
    "    extracts_response = requests.get(WIKIPEDIA_API_ENDPOINT, headers=HEADERS, params=PARAMS_GETCONTENT)\n",
    "    extracts_data = extracts_response.json()\n",
    "    html_content = next(iter(extracts_data[\"query\"][\"pages\"].values()))[\"extract\"]\n",
    "    content = BeautifulSoup(html_content, 'html.parser')\n",
    "    if content.find(\"p\", {\"class\":\"mw-empty-elt\"}):\n",
    "        content.find(\"p\", {\"class\":\"mw-empty-elt\"}).decompose()\n",
    "    return str(content)\n",
    "\n",
    "print(get_wikipedia_content(\"Q7143512\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_dict = {\"gender\": \"P21\", \"birth_date\": \"P569\", \"birth_city\": \"P19\", \n",
    "                  \"birth_country\": \"P17\", \"employer\": \"P108\", \"educated_at\": \"P69\"}\n",
    "\n",
    "def get_wikidata_label(entity_id):\n",
    "    PARAMS_WBGETENTITIES_LABELS[\"ids\"] = entity_id\n",
    "    wbgetentities_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_WBGETENTITIES_LABELS)\n",
    "    wbgetentities_data = wbgetentities_response.json()\n",
    "    labels = next(iter(wbgetentities_data[\"entities\"].values()))[\"labels\"]\n",
    "    value = labels[\"en\"][\"value\"]\n",
    "    return value\n",
    "\n",
    "def get_wikidata_claims(entity_id):\n",
    "    PARAMS_WBGETENTITIES_CLAIMS[\"ids\"] = entity_id\n",
    "    wbgetentities_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_WBGETENTITIES_CLAIMS)\n",
    "    wbgetentities_data = wbgetentities_response.json()\n",
    "    claims = next(iter(wbgetentities_data[\"entities\"].values()))[\"claims\"]\n",
    "    return claims\n",
    "\n",
    "def get_dict_values(entity_id):\n",
    "    claims = get_wikidata_claims(entity_id)\n",
    "    try:\n",
    "        name = get_wikidata_label(entity_id).split(\" (\")[0]\n",
    "    except KeyError:\n",
    "        name = None\n",
    "    try:\n",
    "        intro = get_wikipedia_content(entity_id).split(\"<h2>\")[0]\n",
    "    except KeyError:\n",
    "        intro = None   \n",
    "    try: \n",
    "        gender = get_wikidata_label(claims[wiki_data_dict[\"gender\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "    except KeyError:\n",
    "        gender = None\n",
    "    try:\n",
    "        birth_date = datetime.strptime(claims[wiki_data_dict[\"birth_date\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"], \"+%Y-%m-%dT%XZ\").strftime(\"%d %B %Y\")\n",
    "    except ValueError:\n",
    "        birth_date = datetime.strptime(claims[wiki_data_dict[\"birth_date\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"], \"+%Y-00-00T%XZ\").strftime(\"%Y\")\n",
    "    except KeyError:\n",
    "        birth_date = None\n",
    "    try:\n",
    "        birth_city = get_wikidata_label(claims[wiki_data_dict[\"birth_city\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "        birth_city_claims = get_wikidata_claims(claims[wiki_data_dict[\"birth_city\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "        birth_country = get_wikidata_label(birth_city_claims[wiki_data_dict[\"birth_country\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "        birth_place = \"{}, {}\".format(birth_city, birth_country)\n",
    "    except KeyError:\n",
    "        birth_place = None\n",
    "    try:\n",
    "        employer_list = []\n",
    "        for i in range(len(claims[wiki_data_dict[\"employer\"]])):\n",
    "            employer = get_wikidata_label(claims[wiki_data_dict[\"employer\"]][i][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "            employer_list.append(employer)\n",
    "        employer = employer_list\n",
    "    except KeyError:\n",
    "        employer = None\n",
    "    try:\n",
    "        education_list = []\n",
    "        for i in range(len(claims[wiki_data_dict[\"educated_at\"]])):\n",
    "            education = get_wikidata_label(claims[wiki_data_dict[\"educated_at\"]][i][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "            education_list.append(education)\n",
    "        education = education_list\n",
    "    except KeyError:\n",
    "        education = None\n",
    "    return name, intro, gender, birth_date, birth_place, employer, education\n",
    "\n",
    "\n",
    "award_winners = {\"name\": [], \"intro\": [], \"gender\": [], \"birth_date\": [], \n",
    "                 \"birth_place\": [], \"employer\": [], \"educated_at\": []}\n",
    "acm_award_winners = get_turing_award_recipients()\n",
    "for entity_id in acm_award_winners:\n",
    "    name, intro, gender, birth_date, birth_place, employer, education = get_dict_values(entity_id)\n",
    "    award_winners[\"name\"].append(name)\n",
    "    award_winners[\"intro\"].append(intro)\n",
    "    award_winners[\"gender\"].append(gender)\n",
    "    award_winners[\"birth_date\"].append(birth_date)\n",
    "    award_winners[\"birth_place\"].append(birth_place)\n",
    "    award_winners[\"employer\"].append(employer)\n",
    "    award_winners[\"educated_at\"].append(education)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in sorted(award_winners[\"name\"]):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners_intro = pd.DataFrame(columns=[\"winner_name\", \"count_words\", \"count_sentences\", \"count_paragraphs\", \"common_words\"])\n",
    "award_winners_intro[\"winner_name\"] = award_winners[\"name\"]\n",
    "\n",
    "def get_intro_stats(intro): \n",
    "    intro_html = BeautifulSoup(intro, \"html.parser\")\n",
    "    intro_text = intro_html.get_text(\" \")\n",
    "    count_words = sum([word.strip(string.punctuation).isalnum() for word in intro_text.split()])\n",
    "    count_sentences = len(sent_tokenize(intro_text))\n",
    "    count_paragraphs = len(intro_html.find_all(\"p\"))\n",
    "    word_freqdist= nltk.FreqDist([word for word in word_tokenize(intro_text) if word not in list(string.punctuation)]).most_common(10)\n",
    "    common_words = [word[0] for word in word_freqdist]\n",
    "    return count_words, count_sentences, count_paragraphs, common_words\n",
    "\n",
    "intro_stats = {\"word_count\": [], \"sentence_count\": [], \"paragraph_count\": [], \"common_words\": []}\n",
    "for intro in award_winners[\"intro\"]:\n",
    "    count_word, count_sentences, count_paragraphs, common_words = get_intro_stats(intro)\n",
    "    intro_stats[\"word_count\"].append(count_word)\n",
    "    intro_stats[\"sentence_count\"].append(count_sentences)\n",
    "    intro_stats[\"paragraph_count\"].append(count_paragraphs)\n",
    "    intro_stats[\"common_words\"].append(common_words)\n",
    "\n",
    "award_winners_intro[\"count_words\"] = intro_stats[\"word_count\"]\n",
    "award_winners_intro[\"count_sentences\"] = intro_stats[\"sentence_count\"]\n",
    "award_winners_intro[\"count_paragraphs\"] = intro_stats[\"paragraph_count\"]\n",
    "award_winners_intro[\"common_words\"] = intro_stats[\"common_words\"]\n",
    "\n",
    "# award_winners_intro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       winner_name  count_words  count_sentences  count_paragraphs  \\\n",
      "0  Tim Berners-Lee          348               17                 4   \n",
      "1    Yoshua Bengio           90                4                 2   \n",
      "2  Geoffrey Hinton          176                8                 3   \n",
      "3     Donald Knuth          182                8                 3   \n",
      "4  Richard M. Karp           91                3                 2   \n",
      "5    Robert Tarjan           59                3                 1   \n",
      "6        Vint Cerf           62                2                 1   \n",
      "7      Judea Pearl          154                5                 2   \n",
      "8    Herbert Simon          175                7                 2   \n",
      "9    Marvin Minsky           51                2                 2   \n",
      "\n",
      "                                        common_words  \\\n",
      "0    [the, of, and, Web, He, a, is, as, World, Wide]   \n",
      "1  [and, the, of, Bengio, for, is, a, work, deep,...   \n",
      "2    [the, and, of, for, in, Hinton, a, his, to, is]   \n",
      "3  [the, of, and, Knuth, computer, is, to, He, sc...   \n",
      "4  [in, and, the, of, for, Karp, is, computer, th...   \n",
      "5  [and, Tarjan, is, the, of, University, at, Rob...   \n",
      "6  [the, of, and, is, Internet, National, Medal, ...   \n",
      "7   [the, and, for, of, Pearl, is, on, a, in, Judea]   \n",
      "8  [the, of, and, was, in, science, to, political...   \n",
      "9  [and, of, AI, Minsky, the, Marvin, Lee, August...   \n",
      "\n",
      "                    common_words_after_preprocessing  \n",
      "0  [Web, He, World, Wide, Berners-Lee, 's, Comput...  \n",
      "1  [Bengio, work, deep, learning, Learning, Hinto...  \n",
      "2  [Hinton, computer, work, neural, networks, Goo...  \n",
      "3  [Knuth, computer, He, science, analysis, algor...  \n",
      "4  [Karp, computer, theory, algorithms, Richard, ...  \n",
      "5  [Tarjan, University, Robert, Endre, born, Apri...  \n",
      "6  [Internet, National, Medal, Vinton, Gray, Cerf...  \n",
      "7  [Pearl, Judea, computer, probabilistic, artifi...  \n",
      "8  [science, political, computer, He, Simon, 2001...  \n",
      "9  [AI, Minsky, Marvin, Lee, August, 9, 1927, Jan...  \n"
     ]
    }
   ],
   "source": [
    "def process_common_words(intro):\n",
    "    intro_text = BeautifulSoup(intro, \"html.parser\").get_text(\" \")\n",
    "    word_filter = set(stopwords.words('english') + list(string.punctuation) + [\"``\", \"''\", \"–\"])\n",
    "    tokenized_intro = word_tokenize(intro_text)\n",
    "    word_freqdist = nltk.FreqDist([word for word in tokenized_intro if word not in word_filter]).most_common(10)\n",
    "    common_words = [word[0] for word in word_freqdist]\n",
    "    return common_words\n",
    "\n",
    "common_words_after_preprocessing = [process_common_words(intro) for intro in award_winners[\"intro\"]]\n",
    "award_winners_intro[\"common_words_after_preprocessing\"] = common_words_after_preprocessing\n",
    "\n",
    "print(award_winners_intro.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Sub Activity\n",
    "\n",
    "Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_intro_text = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "\n",
    "word_filter = set(stopwords.words('english') + list(string.punctuation) + [\"``\", \"''\", \"–\"])\n",
    "tokenized_intro = word_tokenize(long_intro_text)\n",
    "intro_words = [word for word in tokenized_intro if word not in word_filter]\n",
    "\n",
    "print(\"Number of unique words before stemming with Porter Stemmer: {}\".format(len(list(nltk.FreqDist(intro_words)))))\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "intro_words = [porter_stemmer.stem(word) for word in intro_words]\n",
    "\n",
    "print(\"Number of unique words after stemming with Porter Stemmer: {}\".format(len(list(nltk.FreqDist(intro_words)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_intro_text = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "\n",
    "word_filter = set(stopwords.words('english') + list(string.punctuation) + [\"``\", \"''\", \"–\"])\n",
    "tokenized_intro = word_tokenize(long_intro_text)\n",
    "intro_words = [word for word in tokenized_intro if word not in word_filter]\n",
    "\n",
    "print(\"Number of unique words before stemming with Snowball Stemmer: {}\".format(len(list(nltk.FreqDist(intro_words)))))\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "intro_words = [snow_stemmer.stem(word) for word in intro_words]\n",
    "\n",
    "print(\"Number of unique words after stemming with Snowball Stemmer: {}\".format(len(list(nltk.FreqDist(intro_words)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words before lemmatization with Word Net Lemmatizer: 1766\n",
      "Number of unique words after lemmatization with Word Net Lemmatizer: 1715\n"
     ]
    }
   ],
   "source": [
    "long_intro_text = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "\n",
    "word_filter = set(stopwords.words('english') + list(string.punctuation) + [\"``\", \"''\", \"–\"])\n",
    "tokenized_intro = word_tokenize(long_intro_text)\n",
    "intro_words = [word for word in tokenized_intro if word not in word_filter]\n",
    "\n",
    "print(\"Number of unique words before lemmatization with Word Net Lemmatizer: {}\".format(len(list(nltk.FreqDist(intro_words)))))\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "intro_words = [wordnet_lemmatizer.lemmatize(word) for word in intro_words]\n",
    "\n",
    "print(\"Number of unique words after lemmatization with Word Net Lemmatizer: {}\".format(len(list(nltk.FreqDist(intro_words)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       winner_name  count_words  count_sentences  count_paragraphs  \\\n",
      "0  Tim Berners-Lee          348               17                 4   \n",
      "1    Yoshua Bengio           90                4                 2   \n",
      "2  Geoffrey Hinton          176                8                 3   \n",
      "3     Donald Knuth          182                8                 3   \n",
      "4  Richard M. Karp           91                3                 2   \n",
      "5    Robert Tarjan           59                3                 1   \n",
      "6        Vint Cerf           62                2                 1   \n",
      "7      Judea Pearl          154                5                 2   \n",
      "8    Herbert Simon          175                7                 2   \n",
      "9    Marvin Minsky           51                2                 2   \n",
      "\n",
      "                                        common_words  \\\n",
      "0    [the, of, and, Web, He, a, is, as, World, Wide]   \n",
      "1  [and, the, of, Bengio, for, is, a, work, deep,...   \n",
      "2    [the, and, of, for, in, Hinton, a, his, to, is]   \n",
      "3  [the, of, and, Knuth, computer, is, to, He, sc...   \n",
      "4  [in, and, the, of, for, Karp, is, computer, th...   \n",
      "5  [and, Tarjan, is, the, of, University, at, Rob...   \n",
      "6  [the, of, and, is, Internet, National, Medal, ...   \n",
      "7   [the, and, for, of, Pearl, is, on, a, in, Judea]   \n",
      "8  [the, of, and, was, in, science, to, political...   \n",
      "9  [and, of, AI, Minsky, the, Marvin, Lee, August...   \n",
      "\n",
      "                    common_words_after_preprocessing  \\\n",
      "0  [Web, He, World, Wide, Berners-Lee, 's, Comput...   \n",
      "1  [Bengio, work, deep, learning, Learning, Hinto...   \n",
      "2  [Hinton, computer, work, neural, networks, Goo...   \n",
      "3  [Knuth, computer, He, science, analysis, algor...   \n",
      "4  [Karp, computer, theory, algorithms, Richard, ...   \n",
      "5  [Tarjan, University, Robert, Endre, born, Apri...   \n",
      "6  [Internet, National, Medal, Vinton, Gray, Cerf...   \n",
      "7  [Pearl, Judea, computer, probabilistic, artifi...   \n",
      "8  [science, political, computer, He, Simon, 2001...   \n",
      "9  [AI, Minsky, Marvin, Lee, August, 9, 1927, Jan...   \n",
      "\n",
      "                                            synonyms  \\\n",
      "0  [web, web, entanglement, vane, web, network, w...   \n",
      "1  [work, work, piece_of_work, employment, work, ...   \n",
      "2  [computer, computing_machine, computing_device...   \n",
      "3  [computer, computing_machine, computing_device...   \n",
      "4  [computer, computing_machine, computing_device...   \n",
      "5  [university, university, university, Robert, H...   \n",
      "6  [internet, net, cyberspace, national, subject,...   \n",
      "7  [pearl, bone, ivory, pearl, off-white, drop, b...   \n",
      "8  [science, scientific_discipline, skill, scienc...   \n",
      "9  [Army_Intelligence, AI, artificial_intelligenc...   \n",
      "\n",
      "                                           antonyms  \n",
      "0            [narrow, narrow, middle, last, second]  \n",
      "1     [idle, malfunction, shallow, shallow, unborn]  \n",
      "2             [idle, malfunction, shallow, shallow]  \n",
      "3                                       [synthesis]  \n",
      "4                                          [unborn]  \n",
      "5                                          [unborn]  \n",
      "6                    [international, local, unborn]  \n",
      "7  [natural, stupidity, devolution, nondevelopment]  \n",
      "8                                    [nonpolitical]  \n",
      "9                                        [windward]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210/3179988525.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  award_winners_intro[\"synonyms\"][i] = synonyms\n",
      "/tmp/ipykernel_210/3179988525.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  award_winners_intro[\"antonyms\"][i] = antonyms\n"
     ]
    }
   ],
   "source": [
    "award_winners_intro = award_winners_intro.assign(synonyms = np.nan, antonyms = np.nan)\n",
    "common_words_after_preprocessing = award_winners_intro[\"common_words_after_preprocessing\"]\n",
    "\n",
    "def get_synonyms_antonyms(common_words):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    for word in common_words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "                if l.antonyms():\n",
    "                    antonyms.append(l.antonyms()[0].name())\n",
    "    return synonyms, antonyms\n",
    "\n",
    "for common_words in common_words_after_preprocessing:\n",
    "    synonyms, antonyms = get_synonyms_antonyms(common_words_after_preprocessing[i])\n",
    "    award_winners_intro[\"synonyms\"][i] = synonyms\n",
    "    award_winners_intro[\"antonyms\"][i] = antonyms\n",
    "\n",
    "print(award_winners_intro.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "334b9cbccb391e97a610fa1686459925ba3c38a6dfd8a581da5ac585bc22844f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
