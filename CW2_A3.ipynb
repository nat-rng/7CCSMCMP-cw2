{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports and Global Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from datetime import datetime\n",
    "from urllib.parse import unquote\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures \n",
    "from concurrent.futures import ThreadPoolExecutor, ALL_COMPLETED\n",
    "from collections import OrderedDict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# API Endpoints for Wikipedia and Wikidata\n",
    "WIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "WIKIPEDIA_API_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# API Header to identify requestor for Wikipedia and Wikidata\n",
    "HEADERS = {\"User-Agent\": \"uni_coursework/2.0; nat.1.roongjirarat@kcl.ac.uk\"}\n",
    "\n",
    "# API Parameters for Wikipedia and Wikidata to get list of award winners, contents of award winners' Wikipedia pages, \n",
    "# labels of award winners, Wikipedia URLs of award winners, and claims of award winners (which contains the entity properties)\n",
    "PARAMS_QUERY_SEARCH = {\n",
    "    \"action\":\"query\",\n",
    "    \"format\":\"json\",\n",
    "    \"formatversion\":\"latest\",\n",
    "    \"list\":\"search\",\n",
    "    \"srsearch\": \"haswbstatement:P166=Q185667\",\n",
    "    \"srlimit\":\"max\"\n",
    "}\n",
    "# Get the contents of the award winners' Wikipedia pages\n",
    "PARAMS_GETCONTENT = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",\n",
    "    \"prop\": \"extracts\",\n",
    "    \"exlimit\": \"max\"\n",
    "}\n",
    "# Get the labels of the award winners. These are the names of the award winners, place of birth, employer, etc.\n",
    "PARAMS_WBGETENTITIES_LABELS = {\n",
    "    \"action\": \"wbgetentities\",\n",
    "    \"format\": \"json\",\n",
    "    \"ids\": \"\",\n",
    "    \"sites\": \"\",\n",
    "    \"props\": \"labels\",\n",
    "    \"languages\": \"en\",\n",
    "    \"sitefilter\": \"enwiki\",\n",
    "    \"utf8\": 1,\n",
    "    \"ascii\": 1,\n",
    "    \"formatversion\": \"latest\"\n",
    "}\n",
    "# Get the Wikipedia URLs/name of the award winners\n",
    "PARAMS_WBGETENTITIES_SITES = {\n",
    "    \"action\": \"wbgetentities\",\n",
    "    \"format\": \"json\",\n",
    "    \"ids\": \"\",\n",
    "    \"sites\": \"\",\n",
    "    \"props\": \"sitelinks/urls\",\n",
    "    \"languages\": \"en\",\n",
    "    \"sitefilter\": \"enwiki\",\n",
    "    \"utf8\": 1,\n",
    "    \"ascii\": 1,\n",
    "    \"formatversion\": \"latest\"\n",
    "}\n",
    "# Get the claims of the award winners. Tese are the property fields such as name, birth date, birth place, etc.\n",
    "PARAMS_WBGETENTITIES_CLAIMS = {\n",
    "    \"action\": \"wbgetentities\",\n",
    "    \"format\": \"json\",\n",
    "    \"ids\": \"\",\n",
    "    \"props\": \"claims\",\n",
    "    \"languages\": \"en\",\n",
    "    \"sitefilter\": \"\",\n",
    "    \"formatversion\": \"latest\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Activity 3 </h2>\n",
    "\n",
    "<h3>3.1 Sub-activity: Loading and pre-processing of text data</h3>\n",
    "\n",
    "<h4>Task 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_turing_award_recipients():\n",
    "    \"\"\"\n",
    "    Function to return a list of Turing Award winners from Wikidata.\n",
    "\n",
    "    Returns:\n",
    "        list: Entity IDs of Turing Award winners, e.g. ['Q12345', 'Q23456', ...]\n",
    "        this list is used in Task 3 to iterate over the list of Turing Award winners and get their contents from Wikipedia\n",
    "        and properties from Wikidata.\n",
    "    \"\"\"\n",
    "    acm_award_entities = []\n",
    "    search_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_QUERY_SEARCH)\n",
    "    data = search_response.json()\n",
    "    for result in data['query']['search']:\n",
    "        acm_award_entities.append(result['title'])\n",
    "    return acm_award_entities\n",
    "\n",
    "print(get_turing_award_recipients())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 2<h/4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(entity_id):\n",
    "    \"\"\"\n",
    "    Takes an enity ID or entity IDs (in the format Q111|Q111|...) of a Turing Award winner and returns the \n",
    "    content of the Wikipedia page of the Turing Award winner.\n",
    "\n",
    "    Args:\n",
    "        entity_id (str): The queery string of entity IDs, e.g. 'Q12345|Q23456|Q34567' or 'Q12345'\n",
    "\n",
    "    Returns:\n",
    "        str: Returns the HTML content of the Wikipedia page of the Turing Award winner/s as a string.\n",
    "    \"\"\"\n",
    "    PARAMS_WBGETENTITIES_SITES[\"ids\"] = entity_id\n",
    "    wbgetentities_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_WBGETENTITIES_SITES)\n",
    "    wbgetentities_data = wbgetentities_response.json()\n",
    "    recipient_name = wbgetentities_data[\"entities\"][entity_id][\"sitelinks\"][\"enwiki\"][\"url\"].split(\"https://en.wikipedia.org/wiki/\")[1]\n",
    "    try:\n",
    "        PARAMS_GETCONTENT[\"titles\"] = unquote(recipient_name)\n",
    "        extracts_response = requests.get(WIKIPEDIA_API_ENDPOINT, headers=HEADERS, params=PARAMS_GETCONTENT)\n",
    "        extracts_data = extracts_response.json()\n",
    "        html_content = next(iter(extracts_data[\"query\"][\"pages\"].values()))[\"extract\"]\n",
    "        content = BeautifulSoup(html_content, 'html.parser')\n",
    "        if content.find(\"p\", {\"class\":\"mw-empty-elt\"}):\n",
    "            content.find(\"p\", {\"class\":\"mw-empty-elt\"}).decompose()\n",
    "        return str(content)\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intro_data(entity_ids):\n",
    "    \"\"\"\n",
    "    Takes a list of entity IDs of Turing Award winners and returns a dictionary of the intro of the Wikipedia page of the Turing Award winner/s.\n",
    "\n",
    "    Args:\n",
    "        entity_ids (list): List of entity IDs of Turing Award winners, e.g. ['Q12345', 'Q23456', ...] to retrieve the intro content of the Wikipedia\n",
    "        page of the Turing Award winner/s.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of the intro of the Wikipedia page of the Turing Award winner/s, with entity ID as the key and intro as the value,\n",
    "        e.g. {'Q12345': 'This is the intro of Q12345', 'Q23456': 'This is the intro of Q23456', ...}\n",
    "    \"\"\"\n",
    "    content_dict = dict()\n",
    "    for entity_id in entity_ids:\n",
    "        content_dict[entity_id] = get_wikipedia_content(entity_id).split(\"<h2>\")[0]\n",
    "    return content_dict\n",
    "\n",
    "def get_wikidata_claims(entity_id):\n",
    "    \"\"\"\n",
    "    Takes an entity ID of a Turing Award winner and returns the claims of the Wikidata page of the Turing Award winner.\n",
    "    Claims are the properties of the Wikidata page of the Turing Award winner.\n",
    "    \n",
    "    Args:\n",
    "        entity_id (str): The entity ID as a query string: 'Q12345'\n",
    "        \n",
    "    Returns:\n",
    "        dict: Returns the claims of the Wikidata page of the Turing Award winner as a dictionary. The dictionary contains the \n",
    "        property ID as the key and property ID as the value.\n",
    "    \"\"\"\n",
    "    PARAMS_WBGETENTITIES_CLAIMS[\"ids\"] = entity_id\n",
    "    wbgetentities_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_WBGETENTITIES_CLAIMS)\n",
    "    wbgetentities_data = wbgetentities_response.json()\n",
    "    claims = next(iter(wbgetentities_data[\"entities\"].values()))[\"claims\"]\n",
    "    return claims\n",
    "\n",
    "def get_wikidata_label(entity_query):\n",
    "    \"\"\"\n",
    "    Takes an property ID or property IDs (in the format P111|P111|...) of a Turing Award winner and returns the label of the \n",
    "    Wikidata page corresponding to the property ID.\n",
    "    \n",
    "    Args:\n",
    "        entity_query (str): The query string of property IDs, e.g. 'P12345|P23456|P34567' or 'P12345'\n",
    "        \n",
    "    Returns:\n",
    "        dict: Returns the labels of the Wikidata page of the Turing Award winner/s as a dictionary. The dictionary contains the\n",
    "        property ID as the key and the label as the value.\n",
    "    \"\"\"\n",
    "    PARAMS_WBGETENTITIES_LABELS[\"ids\"] = entity_query\n",
    "    request_ids = set(entity_query.split(\"|\"))\n",
    "    request_labels = dict()\n",
    "    wbgetentities_response = requests.get(WIKIDATA_API_ENDPOINT, headers=HEADERS, params=PARAMS_WBGETENTITIES_LABELS)\n",
    "    wbgetentities_data = wbgetentities_response.json()\n",
    "    for request_id in request_ids:\n",
    "        labels = wbgetentities_data[\"entities\"][request_id][\"labels\"]\n",
    "        request_labels[request_id] = labels[\"en\"][\"value\"]\n",
    "    return request_labels\n",
    "\n",
    "def check_key_exists(dict, key):\n",
    "    \"\"\"\n",
    "    Verifies if a key exists in a Wikidata claims dictionary.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): Input dictionary to verify if key exists.\n",
    "        key (key): Key to check\n",
    "\n",
    "    Returns:\n",
    "        if Key exists: \n",
    "            list: List of all labels for a given property ID in the Wikidata claims dictionary.\n",
    "        else    \n",
    "            None: Return as None if key does not exist in dictionary.\n",
    "    \"\"\"\n",
    "    if key in dict.keys():\n",
    "        entity_ids = [dict[key][i][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for i in range(len(dict[key]))]\n",
    "        return entity_ids\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def call_wikidata_api(entity_id, content_dict):\n",
    "    \"\"\"\n",
    "    Cal the wikidata API to retrieve the Wikidata labels for all properrties in the Turing Award winner Wikidata Page.\n",
    "    Also retrieves the intro of the Wikipedia page of the Turing Award winner from a content dictionary.\n",
    "    Args:\n",
    "        entity_id (str): Entity ID of the Turing Award winner.\n",
    "        content_dict (dict): Content dictionary of the intros from the Wikipedia pages of the Turing Award winners.\n",
    "\n",
    "    Returns:\n",
    "        name (str): Name of the Turing Award winner.\n",
    "        intro (str): Intro of the Wikipedia page of the Turing Award winner as an HTML string.\n",
    "        gender (str): Gender of the Turing Award winner.\n",
    "        birth_date (DateTime): Birth date of the Turing Award winner as a datetime object.\n",
    "        birth_city (str): Birth city of the Turing Award winner.\n",
    "        employers (list): List of employers of the Turing Award winner. Ordered dictionary is used to remove duplicates and maintain ordering.\n",
    "        educated_at (list): List of institutions the Turing Award winner was educated at. Ordered dictionary is used to remove duplicates and maintain ordering.\n",
    "    \"\"\"\n",
    "    wiki_data_dict = {\"gender\": \"P21\", \"birth_date\": \"P569\", \"birth_city\": \"P19\", \n",
    "                  \"birth_country\": \"P17\", \"employer\": \"P108\", \"educated_at\": \"P69\"}\n",
    "    claims = get_wikidata_claims(entity_id)\n",
    "    request_entityids = {\"name_id\": [entity_id], \"gender_id\": check_key_exists(claims, wiki_data_dict[\"gender\"]),\n",
    "                       \"birth_city_id\" : check_key_exists(claims, wiki_data_dict[\"birth_city\"]),\n",
    "                       \"employers_ids\": check_key_exists(claims, wiki_data_dict[\"employer\"]),\n",
    "                       \"educated_at_ids\": check_key_exists(claims, wiki_data_dict[\"educated_at\"])}\n",
    "    \n",
    "    \n",
    "    entity_query = \"|\".join([\"|\".join(values) for values in request_entityids.values() if values != None])\n",
    "    request_labels = get_wikidata_label(entity_query)\n",
    "    \n",
    "    try:\n",
    "        name = request_labels[entity_id].split(\" (\")[0]\n",
    "    except TypeError:\n",
    "        name = None\n",
    "    intro = content_dict[entity_id]\n",
    "    try: \n",
    "        gender = request_labels[request_entityids[\"gender_id\"][0]]\n",
    "    except TypeError:\n",
    "        gender = None\n",
    "    try:\n",
    "        birth_date = datetime.strptime(claims[wiki_data_dict[\"birth_date\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"], \"+%Y-%m-%dT%XZ\").strftime(\"%d %B %Y\")\n",
    "    except ValueError:\n",
    "        birth_date = datetime.strptime(claims[wiki_data_dict[\"birth_date\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"], \"+%Y-00-00T%XZ\").strftime(\"%Y\")\n",
    "    except TypeError:\n",
    "        birth_date = None\n",
    "    try:\n",
    "        birth_place = request_labels[request_entityids[\"birth_city_id\"][0]] \n",
    "    except TypeError:\n",
    "        birth_place = None\n",
    "    if request_entityids[\"employers_ids\"] != None:\n",
    "        employer = list(OrderedDict.fromkeys([request_labels[key] for key in request_entityids[\"employers_ids\"]]))\n",
    "    else:\n",
    "        employer = None\n",
    "    if request_entityids[\"employers_ids\"] != None:\n",
    "        education = list(OrderedDict.fromkeys([request_labels[key] for key in request_entityids[\"educated_at_ids\"]]))\n",
    "    else:\n",
    "        education = None\n",
    "    return name, intro, gender, birth_date, birth_place, employer, education\n",
    "\n",
    "def multi_thread_api_call(intro_dict, workers=10):\n",
    "    \"\"\"\n",
    "    Multi Threaded API call to retrieve the Wikidata labels for all properties in the Turing Award winner Wikidata Page and \n",
    "    corresponding Wikipedia page. Defaults to 10 workers to make concourrent API calls. If a worker fails, it will retry the API call until\n",
    "    all API calls are successful.\n",
    "    \n",
    "    Args:\n",
    "        intro_dict (dict): Dicitionary of the intros from the Wikipedia pages of the Turing Award winners. Where the key is the entity ID and the value is the intro.\n",
    "        workers (int, optional): Number of workers to make concurrnet API calls. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list: The result of the API call for each Turing Award winner. Each item in the list is a tuple with the following structure:\n",
    "        (name, intro, gender, birth_date, birth_place, employer, education)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        api_calls = {executor.submit(call_wikidata_api, entity_id, intro_dict): entity_id for entity_id in intro_dict.keys()}\n",
    "        while api_calls:\n",
    "            retry_api_calls = {}\n",
    "            done = concurrent.futures.wait(api_calls, return_when=ALL_COMPLETED)\n",
    "            for api_call in done:\n",
    "                for future in api_call:\n",
    "                    if future.exception():\n",
    "                        entity_id = api_calls[future]\n",
    "                        retry_api_calls[executor.submit(call_wikidata_api, entity_id, intro_dict)] = entity_id\n",
    "                    else:\n",
    "                        results.append(future.result())\n",
    "            api_calls = retry_api_calls   \n",
    "    return results\n",
    "\n",
    "def get_award_winners_data(results):\n",
    "    \"\"\"\n",
    "    Takes an input list containing the results of the API call for each Turing Award winner and returns a dictionary with the following structure:\n",
    "    {\"name\": [], \"intro\": [], \"gender\": [], \"birth_date\": [], \"birth_place\": [], \"employer\": [], \"educated_at\": []} \n",
    "    \n",
    "    Args:\n",
    "        results (tuple): Tuple with the following format (name, intro, gender, birth_date, birth_place, employer, education)\n",
    "        to unpack and append to each list in the dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of all award winners with the following structure, order is dependent on the API call execution order\n",
    "        eeach value is stored in a list: \n",
    "        {\"name\": [], \"intro\": [], \"gender\": [], \"birth_date\": [], \"birth_place\": [], \"employer\": [], \"educated_at\": []} \n",
    "        \n",
    "    See Also:\n",
    "        def multi_thread_api_call(intro_dict, workers=10)\n",
    "    \"\"\"\n",
    "    award_winners_dict = {\"name\": [], \"intro\": [], \"gender\": [], \"birth_date\": [], \n",
    "                      \"birth_place\": [], \"employer\": [], \"educated_at\": []} \n",
    "    for result in results:\n",
    "        name, intro, gender, birth_date, birth_place, employer, education = result\n",
    "        award_winners_dict[\"name\"].append(name)\n",
    "        award_winners_dict[\"intro\"].append(intro)\n",
    "        award_winners_dict[\"gender\"].append(gender)\n",
    "        award_winners_dict[\"birth_date\"].append(birth_date)\n",
    "        award_winners_dict[\"birth_place\"].append(birth_place)\n",
    "        award_winners_dict[\"employer\"].append(employer)\n",
    "        award_winners_dict[\"educated_at\"].append(education)\n",
    "    return award_winners_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of Turing Award winners by Entity ID\n",
    "acm_award_winners = get_turing_award_recipients()\n",
    "# Get the intro from the Wikipedia page for each Turing Award winner as a dictionary\n",
    "winner_intros = get_intro_data(acm_award_winners)\n",
    "# List of tuples returned from the API call for each Turing Award winner (random order)\n",
    "api_calls = multi_thread_api_call(winner_intros)\n",
    "#Dictionary containing the data for each Turing Award winner\n",
    "award_winners = get_award_winners_data(api_calls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 4</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the Turing Award winners in alphabetical order\n",
    "for name in sorted(award_winners[\"name\"]):\n",
    "    print(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 5</h4>\n",
    "\n",
    "<h5> a) and b) </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Panda DataFrame to store the data for each Turing Award winner intro text\n",
    "award_winners_intro = pd.DataFrame(columns=[\"winner_name\", \"count_words\", \"count_sentences\", \"count_paragraphs\", \"common_words\"])\n",
    "# Add name of the Turing Award winner to the DataFrame\n",
    "award_winners_intro[\"winner_name\"] = award_winners[\"name\"]\n",
    "\n",
    "def get_intro_stats(intro):\n",
    "    \"\"\"\n",
    "    Takes as input the intro text as an html string and returns the number of words, sentences, paragraphs and the 10 most common words in the intro.\n",
    "\n",
    "    Args:\n",
    "        intro (str): Intro text as an html string, get processed to remove html tags and punctuation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple with the following structure (count_words, count_sentences, count_paragraphs, common_words). \n",
    "        Contains the number of words, sentences, paragraphs and the 10 most common words in the intro.\n",
    "    \"\"\"\n",
    "    intro_html = BeautifulSoup(intro, \"html.parser\")\n",
    "    intro_text = intro_html.get_text(\" \").lower()\n",
    "    count_words = sum([word.strip(string.punctuation).isalnum() for word in intro_text.split()])\n",
    "    count_sentences = len(sent_tokenize(intro_text))\n",
    "    count_paragraphs = len(intro_html.find_all(\"p\"))\n",
    "    word_filter = set(list(string.punctuation) + [\"``\", \"''\", \"–\"])\n",
    "    word_freqdist= nltk.FreqDist([word for word in word_tokenize(intro_text) if word not in word_filter]).most_common(10)\n",
    "    common_words = [word[0] for word in word_freqdist]\n",
    "    return count_words, count_sentences, count_paragraphs, common_words\n",
    "\n",
    "# Iterate and get the intro stats for each Turing Award winner, store the results in a dictionary, where the key are the intro stats and the values are lists\n",
    "# for each Turing Award winner\n",
    "intro_stats = {\"word_count\": [], \"sentence_count\": [], \"paragraph_count\": [], \"common_words\": []}\n",
    "for intro in award_winners[\"intro\"]:\n",
    "    count_word, count_sentences, count_paragraphs, common_words = get_intro_stats(intro)\n",
    "    intro_stats[\"word_count\"].append(count_word)\n",
    "    intro_stats[\"sentence_count\"].append(count_sentences)\n",
    "    intro_stats[\"paragraph_count\"].append(count_paragraphs)\n",
    "    intro_stats[\"common_words\"].append(common_words)\n",
    "\n",
    "# Add the intro stats to the DataFrame from the dictionary\n",
    "award_winners_intro[\"count_words\"] = intro_stats[\"word_count\"]\n",
    "award_winners_intro[\"count_sentences\"] = intro_stats[\"sentence_count\"]\n",
    "award_winners_intro[\"count_paragraphs\"] = intro_stats[\"paragraph_count\"]\n",
    "award_winners_intro[\"common_words\"] = intro_stats[\"common_words\"]\n",
    "\n",
    "award_winners_intro.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 5</h4>\n",
    "\n",
    "<h5>c) and d)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_top10_common_words(intro):\n",
    "    \"\"\"\n",
    "    Takes as input the intro text as an html string and returns the 10 most common words in the intro after removing stopwords and punctuation.\n",
    "\n",
    "    Args:\n",
    "        intro (str): Intro text as an html string, gets processed to remove html tags, punctuation and stop words.\n",
    "\n",
    "    Returns:\n",
    "        list: List with the 10 most common words in the intro after removing stopwords and punctuation.\n",
    "    \"\"\"\n",
    "    intro_text = BeautifulSoup(intro, \"html.parser\").get_text(\" \").lower()\n",
    "    word_filter = set(stopwords.words('english') + list(string.punctuation) + [\"``\", \"''\", \"–\"])\n",
    "    tokenized_intro = word_tokenize(intro_text)\n",
    "    word_freqdist = nltk.FreqDist([word for word in tokenized_intro if word not in word_filter]).most_common(10)\n",
    "    common_words = [word[0] for word in word_freqdist]\n",
    "    return common_words\n",
    "\n",
    "# Get the 10 most common words in the intro after removing stopwords and punctuation. Returned as a list of lists\n",
    "common_words_after_preprocessing = [process_top10_common_words(intro) for intro in award_winners[\"intro\"]]\n",
    "# Add the 10 most common words in the intro after removing stopwords and punctuation to the DataFrame\n",
    "award_winners_intro[\"common_words_after_preprocessing\"] = common_words_after_preprocessing\n",
    "# Get the first 10 rows of the DataFrame (order is dependent on the order the rows were called in the API)\n",
    "award_winners_intro.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2 Sub Activity:  Applying NLP operations on the corpus</h3>\n",
    "\n",
    "<h4>3.2.1 Stemming</h4>\n",
    "\n",
    "<h4>Task 3</h4>\n",
    "\n",
    "<h5>a), b) and c)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_intro_words(intro):\n",
    "    \"\"\"\n",
    "    Takes as input the intro text as an html string and returns the intro text after removing stopwords and punctuation.\n",
    "\n",
    "    Args:\n",
    "        intro (str): Intro text as an html string, gets processed to remove html tags, punctuation and stop words.\n",
    "\n",
    "    Returns:\n",
    "        list: List of all words in the intro text after removing stopwords and punctuation. Maintains capitalization of names, lower() method\n",
    "        if applied implicitly in the stemmer methods and explicitly for thee lemmitization.\n",
    "    \"\"\"\n",
    "    word_filter = set(stopwords.words('english') + list(string.punctuation) + [\"``\", \"''\", \"–\"])\n",
    "    tokenized_intro = word_tokenize(intro)\n",
    "    intro_words = [word for word in tokenized_intro if word.lower() not in word_filter]\n",
    "    return intro_words\n",
    "\n",
    "def count_unique_words(intro):\n",
    "    \"\"\"\n",
    "    Takes as input the intro text as a list of words and returns the number of unique words in the intro.\n",
    "\n",
    "    Args:\n",
    "        intro (list): Intro text as a list of words.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of unique words in the intro.\n",
    "    \"\"\"\n",
    "    unique_intro_words = [word.lower() for word in intro]\n",
    "    return len(list(nltk.FreqDist(unique_intro_words)))\n",
    "\n",
    "# Join all intro texts into one long string and process the words to get the number of unique words across all intro texts before stemming\n",
    "long_intro_text = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "intro_words = process_intro_words(long_intro_text)\n",
    "original_unique_words = count_unique_words(intro_words)\n",
    "\n",
    "print(\"Number of unique words before stemming with Porter Stemmer: {}\".format(original_unique_words))\n",
    "\n",
    "# Stem the intro words with Porter Stemmer and get the number of unique words across all intro texts after stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "intro_words_with_porterstemmer = [porter_stemmer.stem(word) for word in intro_words]\n",
    "\n",
    "print(\"Number of unique words after stemming with Porter Stemmer: {}\".format(len(list(nltk.FreqDist(intro_words_with_porterstemmer)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 4</h4>\n",
    "\n",
    "<h5>a), b) and c)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all intro texts into one long string and process the words to get the number of unique words across all intro texts before stemming\n",
    "long_intro_text = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "intro_words = process_intro_words(long_intro_text)\n",
    "original_unique_words = count_unique_words(intro_words)\n",
    "\n",
    "print(\"Number of unique words before stemming with Porter Stemmer: {}\".format(original_unique_words))\n",
    "\n",
    "# Stem the intro words with Snowball Stemmer and get the number of unique words across all intro texts after stemming\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "intro_words_with_snowballstemmer = [snow_stemmer.stem(word) for word in intro_words]\n",
    "\n",
    "print(\"Number of unique words after stemming with Snowball Stemmer: {}\".format(len(list(nltk.FreqDist(intro_words_with_snowballstemmer)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2.2 Lemmatization</h3>\n",
    "\n",
    "<h3>Task 5</h3>\n",
    "\n",
    "<h5>a), b) and c)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all intro texts into one long string and process the words to get the number of unique words across all intro texts before stemming\n",
    "long_intro_text = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "intro_words = process_intro_words(long_intro_text)\n",
    "original_unique_words = count_unique_words(intro_words)\n",
    "\n",
    "print(\"Number of unique words before stemming with Porter Stemmer: {}\".format(original_unique_words))\n",
    "\n",
    "# Lemmatize the intro words with Word Net Lemmatizer and get the number of unique words across all intro texts after lemmatization.\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "intro_words_wordnetlemmatizer = [wordnet_lemmatizer.lemmatize(word.lower()) for word in intro_words]\n",
    "\n",
    "print(\"Number of unique words after lemmatization with Word Net Lemmatizer: {}\".format(len(list(nltk.FreqDist(intro_words_wordnetlemmatizer)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2.3 Finding synonyms and antonyms</h3>\n",
    "\n",
    "<h4>Task 4</h4>\n",
    "\n",
    "<h5>a), b), c) and d)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add synonyms and antonyms columns to the DataFrame\n",
    "award_winners_intro = award_winners_intro.assign(synonyms = np.nan, antonyms = np.nan)\n",
    "# Get all the common words from the preprocessed intro texts from the DataFrame\n",
    "common_words_after_preprocessing = award_winners_intro[\"common_words_after_preprocessing\"]\n",
    "\n",
    "def get_synonyms(common_words):\n",
    "    \"\"\"\n",
    "    Takes as input a list of common words and returns a list of synonyms for the common words.\n",
    "\n",
    "    Args:\n",
    "        common_words (list): List of common words.\n",
    "\n",
    "    Returns:\n",
    "        list: List of synonyms for the common words excluding the word itself. Ordered dictionary is used to remove duplicates and maintain ordering.\n",
    "    \"\"\"\n",
    "    synonyms = [l.name() for word in common_words for syn in wordnet.synsets(word) for l in syn.lemmas() if l.name() != word]\n",
    "    return list(OrderedDict.fromkeys(synonyms))\n",
    "\n",
    "def get_antonyms(common_words):\n",
    "    \"\"\"\n",
    "    Takes as input a list of common words and returns a list of antonyms for the common words.\n",
    "\n",
    "    Args:\n",
    "        common_words (list): List of common words.\n",
    "\n",
    "    Returns:\n",
    "        list: List of antonyms for the common words. Ordered dictionary is used to remove duplicates and maintain ordering.\n",
    "    \"\"\"\n",
    "    antonyms = [l.antonyms()[0].name() for word in common_words for syn in wordnet.synsets(word) for l in syn.lemmas() if l.antonyms()]\n",
    "    return list(OrderedDict.fromkeys(antonyms))\n",
    "\n",
    "#iterate through the list of lists of common words and get the synonyms and antonyms for each word in the list, \n",
    "# outputting the results to the DataFrame's synonyms and antonyms columns\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for common_words in common_words_after_preprocessing:\n",
    "    synonyms_list, antonyms_list = get_synonyms(common_words), get_antonyms(common_words)\n",
    "    synonyms.append(synonyms_list)\n",
    "    antonyms.append(antonyms_list)\n",
    "\n",
    "award_winners_intro[\"synonyms\"] = synonyms\n",
    "award_winners_intro[\"antonyms\"] = antonyms\n",
    "\n",
    "# Get the first 10 rows of the DataFrame (order is dependent on the order the rows were called in the API)\n",
    "award_winners_intro.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.2.4 Bigrams and trigrams</h4>\n",
    "\n",
    "<h4>Task 7</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all intro texts into one long string and process the words by removing stop words and punctuation. \n",
    "# Output the result to a list of words from all intro texts.\n",
    "long_intro_text = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "intro_words = process_intro_words(long_intro_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 8</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_frequency(words_list):\n",
    "    \"\"\"\n",
    "    Takes as input a list of words and returns a dictionary of bigrams as keys and their frequencies as values. Capitalization is maintained \n",
    "    to differentiate between words that are used in different contexts. (E.g. \"Computer Science\" and \"computer science\", to diffeerentiate\n",
    "    between a department/entity of Computer Science and the field of study)\n",
    "\n",
    "    Args:\n",
    "        words_list (list): List of words in the order they appear in the text. (Input text is tokenized and preprocessed in Task 7)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of bigrams as keys and their frequencies as values.\n",
    "    \"\"\"\n",
    "    bigrams = nltk.bigrams(words_list)\n",
    "    bigrams_freqdist = nltk.FreqDist(bigrams)\n",
    "    return dict(bigrams_freqdist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 9</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary of bigrams and their frequencies to a variable called award_winners_bigrams\n",
    "winners_bigrams = get_bigrams_frequency(intro_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 10</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the dictionary of bigrams and their frequencies by their frequencies in descending order and print the top 15 bigrams.\n",
    "# Exact order is dependent on the order the rows were called in the API. In other words, the order of the bigrams with the same frequency is not guaranteed.\n",
    "# To get the exact same order, print more than 15 to verify bigrams with the same frequencies.\n",
    "# Print the top 15 bigrams\n",
    "def get_top_bigrams(bigrams_dict, top_n):\n",
    "    \"\"\"\n",
    "    Takes as input a dictionary of bigrams and their frequencies and returns a dictionary of the top n bigrams and their frequencies.\n",
    "\n",
    "    Args:\n",
    "        bigrams_dict (dict): Dictionary of bigrams as keys and their frequencies as values.\n",
    "        top_n (int): Number of top bigrams to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of top n bigrams as keys and their frequencies as values.\n",
    "    \"\"\"\n",
    "    top_n_bigrams = dict(sorted(bigrams_dict.items(), key=lambda bigram_dict: bigram_dict[1], reverse=True)[:top_n])\n",
    "    return top_n_bigrams\n",
    "\n",
    "top15_bigrams = get_top_bigrams(winners_bigrams, 15)\n",
    "\n",
    "print(\"Top 15 bigrams: {}\".format(\", \".join([str(bigram) for bigram in top15_bigrams.keys()])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.3 Sub-section: Visualisation</h3>\n",
    "\n",
    "<h4>3.3.1 Barplots</h4>\n",
    "\n",
    "<h4>Task 11</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each plot is plotted using the award_winners_intro DataFrame in its own figure\n",
    "# Plot the word counts for each winner's intro text\n",
    "word_comparison = award_winners_intro.plot(x=\"winner_name\", y=\"count_words\", kind='bar', title =\"Comparison of words\", \n",
    "                                           figsize=(20, 15), legend=True)\n",
    "word_comparison.set_xlabel(\"Winners\")\n",
    "word_comparison.set_ylabel(\"Number of Words\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the sentence counts for each winner's intro text\n",
    "sentence_comparison = award_winners_intro.plot(x=\"winner_name\", y=\"count_sentences\", kind='bar', title =\"Comparison of sentences\", \n",
    "                                               figsize=(20, 15), legend=True)\n",
    "sentence_comparison.set_xlabel(\"Winners\")\n",
    "sentence_comparison.set_ylabel(\"Number of Sentences\")\n",
    "plt.xticks(range(len(award_winners_intro[\"winner_name\"])), award_winners_intro[\"winner_name\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot the paragraph counts for each winner's intro text\n",
    "paragraph_comparison = award_winners_intro.plot(x=\"winner_name\", y=\"count_paragraphs\", kind='bar', title =\"Comparison of paragraphs\", \n",
    "                                                figsize=(20, 15), legend=True)\n",
    "paragraph_comparison.set_xlabel(\"Winners\")\n",
    "paragraph_comparison.set_ylabel(\"Number of Paragraphs\")\n",
    "plt.xticks(range(len(award_winners_intro[\"winner_name\"])), award_winners_intro[\"winner_name\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 12</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three comparisons in one figure (3 by 1 subplots)\n",
    "fig, axes = plt.subplots(3, figsize=(20, 50))\n",
    "fig.suptitle(\"Comparison of words, sentences anhd paragraphs\", x = 0.5, y = 0.90, fontsize = 20)\n",
    "\n",
    "# Add plot of word counts for each winner's intro text\n",
    "word_comparison = award_winners_intro.plot(ax = axes[0], x=\"winner_name\", y=\"count_words\", kind='bar', title =\"Comparison of words\", \n",
    "                                           legend=True)\n",
    "word_comparison.set_xlabel(\"Winners\")\n",
    "word_comparison.set_ylabel(\"Number of Words\")\n",
    "\n",
    "# Add plot of sentence counts for each winner's intro text\n",
    "sentence_comparison = award_winners_intro.plot(ax = axes[1], x=\"winner_name\", y=\"count_sentences\", kind='bar', title =\"Comparison of sentences\", \n",
    "                                               legend=True)\n",
    "sentence_comparison.set_xlabel(\"Winners\")\n",
    "sentence_comparison.set_ylabel(\"Number of Sentences\")\n",
    "\n",
    "# Add plot of paragraph counts for each winner's intro text\n",
    "paragraph_comparison = award_winners_intro.plot(ax = axes[2], x=\"winner_name\", y=\"count_paragraphs\", kind='bar', title =\"Comparison of paragraphs\", \n",
    "                                                legend=True)\n",
    "paragraph_comparison.set_xlabel(\"Winners\")\n",
    "paragraph_comparison.set_ylabel(\"Number of Paragraphs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 13</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three comparisons in one figure (3 by 1 subplots)\n",
    "fig, axes = plt.subplots(3, figsize=(20, 60))\n",
    "fig.suptitle(\"Comparison of words, sentences and paragraphs\", x = 0.5, y = 0.90, fontsize = 28)\n",
    "plt.rc('axes', titlesize = 20)\n",
    "text_fontsize = 12\n",
    "label_fontsize = 14\n",
    "\n",
    "# Sort the award_winners_intro DataFrame by the number of words and sentences in ascending order; and paragraphs in descending order\n",
    "sorted_word_counts = award_winners_intro.sort_values(\"count_words\").reset_index(drop=True)\n",
    "sorted_sentence_counts = award_winners_intro.sort_values(\"count_sentences\").reset_index(drop=True)\n",
    "sorted_paragraph_counts = award_winners_intro.sort_values(\"count_paragraphs\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add plot of word counts for each winner's intro text set color to teal and rotate the x-axis labels by 90 degrees\n",
    "word_comparison = sorted_word_counts.plot(ax = axes[0], x=\"winner_name\", y=\"count_words\", kind='bar', title =\"Comparison of word count\", \n",
    "                                           legend=True, color=\"teal\", fontsize=text_fontsize)                              \n",
    "word_comparison.set_xlabel(\"Winners\", fontsize=label_fontsize)\n",
    "word_comparison.set_ylabel(\"Number of Words\", fontsize=label_fontsize)\n",
    "word_comparison.set_xticklabels(sorted_word_counts[\"winner_name\"], rotation=90, ha='right')\n",
    "word_comparison.bar_label(word_comparison.containers[0], rotation = 90, padding = 5, fontsize = text_fontsize)\n",
    "word_comparison.legend(loc=\"upper left\")\n",
    "\n",
    "# Add plot of sentence counts for each winner's intro text and rotate the x-axis labels by 90 degrees\n",
    "sentence_comparison = sorted_sentence_counts.plot(ax = axes[1], x=\"winner_name\", y=\"count_sentences\", kind='bar', title =\"Comparison of sentence count\", \n",
    "                                               legend=True, fontsize=text_fontsize)\n",
    "sentence_comparison.set_xlabel(\"Winners\", fontsize=label_fontsize)\n",
    "sentence_comparison.set_ylabel(\"Number of Sentences\", fontsize=label_fontsize)\n",
    "sentence_comparison.set_xticklabels(sorted_sentence_counts[\"winner_name\"], rotation=90, ha='right')\n",
    "sentence_comparison.bar_label(sentence_comparison.containers[0], rotation = 90, padding = 5, fontsize = text_fontsize)\n",
    "sentence_comparison.legend(loc=\"upper left\")\n",
    "\n",
    "# Add plot of paragraph counts for each winner's intro text and rotate the x-axis labels by 90 degrees. Plot the bars horizontally in descending order\n",
    "paragraph_comparison = sorted_paragraph_counts.plot(ax = axes[2], x=\"winner_name\", y=\"count_paragraphs\", kind='barh', title =\"Comparison of paragraph count\", \n",
    "                                                legend=True, fontsize=text_fontsize)\n",
    "paragraph_comparison.set_xlabel(\"Winners\", fontsize=label_fontsize)\n",
    "paragraph_comparison.set_ylabel(\"Number of Paragraphs\", fontsize=label_fontsize)\n",
    "paragraph_comparison.bar_label(paragraph_comparison.containers[0], padding = 5, fontsize = text_fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.3.2 Heatmap</h4>\n",
    "\n",
    "<h4>Task 14</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency of each bigram and return the top 15 bigrams\n",
    "combined_intro_texts = \" \".join([BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]])\n",
    "combined_intro_words = process_intro_words(combined_intro_texts)\n",
    "winners_bigrams = get_bigrams_frequency(combined_intro_words)\n",
    "\n",
    "# Order in descending order of frequency dependent on the order in which the API returns the results. Particularly\n",
    "# bigrams with the same frequency may not be returned in the same order.\n",
    "top15_bigrams = get_top_bigrams(winners_bigrams, 15)\n",
    "\n",
    "# Process the intro text for each winner to extracts all bigrams in the intro text and store as a list of lists\n",
    "intro_texts = [BeautifulSoup(intro, \"html.parser\").get_text(\" \") for intro in award_winners[\"intro\"]]\n",
    "\n",
    "# Create a list of dictionaries to store the frequency of each bigram for each winner\n",
    "bigram_frequencies = []\n",
    "for intro in intro_texts:\n",
    "    intro_words = process_intro_words(intro)\n",
    "    intro_bigrams = get_bigrams_frequency(intro_words)\n",
    "\n",
    "    top15_intro_bigrams = {key: intro_bigrams[key] for key in top15_bigrams.keys() if key in intro_bigrams.keys()}\n",
    "\n",
    "    bigram_frequencies.append(top15_intro_bigrams)\n",
    "\n",
    "# Create a DataFrame to store the bigram frequencies for each winner. Extracts the bigram frequencies from the list of dictionaries\n",
    "heatmap_df = pd.DataFrame(index = [key for key in top15_bigrams.keys()], columns = award_winners_intro[\"winner_name\"]) \n",
    "for i,winner in enumerate(heatmap_df.columns):\n",
    "    for bigram in bigram_frequencies[i].keys():\n",
    "        heatmap_df[winner][bigram] = bigram_frequencies[i][bigram]\n",
    "\n",
    "# Fill in the NaN values with 0 and plot the heatmap. Order of award winner names is dependent on the API call order\n",
    "heatmap_df.fillna(0, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(80,40))\n",
    "fig.suptitle(\"Bigram Frequency Heatmap\", x = 0.5, y = 0.95, fontsize = 120)\n",
    "bigram_heatmap = sns.heatmap(heatmap_df, annot=True, cmap=\"Blues\", annot_kws={\"fontsize\":40})\n",
    "bigram_heatmap.tick_params(labelsize=40)\n",
    "bigram_heatmap.set_yticklabels(heatmap_df.index, rotation=0)\n",
    "bigram_heatmap.set_xlabel(\"Winners\", fontsize=50)\n",
    "bigram_heatmap.set_ylabel(\"Bigrams\", fontsize=50)\n",
    "bigram_heatmap.collections[0].colorbar.ax.tick_params(labelsize=50)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e2af03db89a63cae038d79569d879ac3706801ba383ac0b3ca91c3e9fed1f36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
